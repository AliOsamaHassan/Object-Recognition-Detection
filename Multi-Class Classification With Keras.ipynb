{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neighbors, preprocessing, metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers import Flatten,Dense,Input,Conv2D,MaxPooling2D,GlobalAveragePooling2D,GlobalMaxPool2D\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialize Random Number Generator\n",
    "\n",
    "Next, we need to initialize the random number generator to a constant value (7).\n",
    "\n",
    "This is important to ensure that the results we achieve from this model can be achieved again precisely. It ensures that the stochastic process of training a neural network model can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_images(raw):\n",
    "    raw_float = np.array(raw, dtype=float) \n",
    "    images = raw_float.reshape([-1, 3, 32, 32])\n",
    "    images = images.transpose([0, 2, 3, 1])\n",
    "    return images\n",
    "\n",
    "def onehot_labels(labels):\n",
    "    return np.eye(100)[labels]\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def print_model(hist):\n",
    "    # list all data in history\n",
    "    print(hist.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(hist.history['acc'])\n",
    "    plt.plot(hist.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'datasets/cifar-100-python/'\n",
    "data = unpickle(dataset_path+'train')\n",
    "X_train = get_proper_images(unpickle(dataset_path+'train')[b'data'])\n",
    "Y_train = onehot_labels(unpickle(dataset_path+'train')[b'fine_labels'])\n",
    "X_test = get_proper_images(unpickle(dataset_path+'test')[b'data'])\n",
    "Y_test = onehot_labels(unpickle(dataset_path+'test')[b'fine_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split_param = 0.1\n",
    "test_size_param = 0.2\n",
    "num_layers_hidden = 2\n",
    "activation_output_function = 'softmax'\n",
    "loss_function_param='categorical_crossentropy'\n",
    "n_units = 32\n",
    "epochs_param = 5\n",
    "batch_size_param = 32\n",
    "load_weights_flag = 1\n",
    "learning_rate = 0.1\n",
    "lr_decay = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define The Convolutional Neural Networks Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 327,332\n",
      "Trainable params: 327,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"input_shape=(32,32,3)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "#saved_weights_name='SVMWeights.h5'\n",
    "#model.load_weights(saved_weights_name)\n",
    "model.summary()\n",
    "\"\"\"\n",
    "\n",
    "#def model():\n",
    "input_shape=(32,32,3)\n",
    "model = Sequential()\n",
    "# Adding the input layer\n",
    "model.add(Conv2D(n_units, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Adding hidden layers\n",
    "for _ in range(num_layers_hidden-1):\n",
    "    model.add(Conv2D(n_units*2, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "model.add(Dense(n_units*4, activation='relu'))\n",
    "# Adding the output layer\n",
    "model.add(Dense(100, activation=activation_output_function))\n",
    "\n",
    "#saved_weights_name=dataset_path+'cifar100vgg.h5'\n",
    "#model.load_weights(saved_weights_name)\n",
    "print(model.summary())\n",
    "#return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "# Compiling Model\n",
    "model.compile(loss=loss_function_param, optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(saved_weights_name, \n",
    "                                     monitor='val_acc', \n",
    "                                     verbose=1, \n",
    "                                     save_best_only=True, \n",
    "                                     mode='max')\n",
    "csv_logger = CSVLogger('myLogFile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 124s 2ms/step - loss: 15.9571 - acc: 0.0100\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 138s 3ms/step - loss: 15.9569 - acc: 0.0100\n",
      "Epoch 3/5\n",
      "15776/50000 [========>.....................] - ETA: 1:30 - loss: 15.9608 - acc: 0.0098  "
     ]
    }
   ],
   "source": [
    "# Fitting Model\n",
    "model_details = model.fit(X_train,Y_train,\n",
    "                          epochs=epochs_param,\n",
    "                          batch_size=batch_size_param,\n",
    "                         validation_split_param=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "model.save('model_file.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model\n",
    "my_model = load_model('model_file.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "predictions = my_model.predict(data_to_predict_with)\n",
    "probability_true = predictions[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    input_shape=(32,32,3)\n",
    "    model = Sequential()\n",
    "    # Adding the input layer\n",
    "    model.add(Conv2D(n_units, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Adding hidden layers\n",
    "    for _ in range(num_layers_hidden-1):\n",
    "        model.add(Conv2D(n_units*2, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(n_units*4, activation='relu'))\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(100, activation=activation_output_function))\n",
    "    #saved_weights_name='SVMWeights.h5'\n",
    "    #model.load_weights(saved_weights_name)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=model, epochs=epochs_param, batch_size=batch_size_param, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate The Model with k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
